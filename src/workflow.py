"""
å·¥ä½œæµç¼–æ’å™¨
åè°ƒæ•´ä¸ªæŠ½å–æµç¨‹çš„æ‰§è¡Œ
"""

import os
import json
import sys
from datetime import datetime
from typing import Dict, List, Optional
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.database_manager_v3 import DatabaseManagerV3
from src.image_manager import ImageManager
from src.template_analyzer import TemplateAnalyzer
from src.prompt_generator import PromptGenerator
from src.artifact_merger import ArtifactMerger
from src.image_linker import ImageLinker
from src.field_mapper import FieldMapper
from src.content_extractor import split_by_tomb
from src.automated_extractor import call_llm_api, extract_json_from_response, load_config


class ExtractionWorkflow:
    """
    æŠ½å–å·¥ä½œæµç¼–æ’å™¨
    åè°ƒæ•´ä¸ªæŠ½å–æµç¨‹
    """
    
    def __init__(self, db_path: str = 'database/artifacts_v3.db'):
        """
        åˆå§‹åŒ–å·¥ä½œæµ
        
        Args:
            db_path: æ•°æ®åº“è·¯å¾„
        """
        self.db = DatabaseManagerV3(db_path)
        self.db.connect()
        
        self.prompt_generator = PromptGenerator()
        self.artifact_merger = ArtifactMerger()
        
        self.llm_config = load_config()
    
    def _check_cancellation(self, task_id: str):
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«ä¸­æ­¢"""
        task = self.db.get_task(task_id)
        if task and task.get('status') == 'aborted':
            self.db.add_log(task_id, 'WARNING', 'æ£€æµ‹åˆ°ä¸­æ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢ä»»åŠ¡...')
            raise Exception("ä»»åŠ¡å·²ç”±ç”¨æˆ·æ‰‹åŠ¨ä¸­æ­¢")

    def execute_full_extraction(self,
                               report_folder: str,
                               templates: Dict[str, str],
                               report_name: Optional[str] = None,
                               bot_id: Optional[str] = None,
                               api_key: Optional[str] = None,
                               task_id: Optional[str] = None) -> str:
        """
        æ‰§è¡Œå®Œæ•´çš„æŠ½å–æµç¨‹
        
        Args:
            report_folder: æŠ¥å‘Šæ–‡ä»¶å¤¹è·¯å¾„
            templates: æ¨¡æ¿æ˜ å°„
            report_name: æŠ¥å‘Šåç§°
            bot_id: æŒ‡å®šä½¿ç”¨çš„ Coze Bot ID
            api_key: æŒ‡å®š Bot å¯¹åº”çš„ API Token
            task_id: ä»»åŠ¡ID (å¦‚æœå·²åˆ›å»ºä»»åŠ¡)
        
        Returns:
            ä»»åŠ¡ID
        """
        # å¦‚æœæä¾›äº† bot_id/api_keyï¼Œæ›´æ–° llm_config
        if bot_id:
            self.llm_config['llm']['bot_id'] = bot_id
            print(f"ğŸ¤– ä½¿ç”¨æŒ‡å®šçš„ Bot ID: {bot_id}")
        
        if api_key:
            self.llm_config['llm']['api_key'] = api_key
            # print(f"ğŸ”‘ ä½¿ç”¨æŒ‡å®šçš„ API Key: {api_key[:4]}...")

        # 1. åˆ›å»ºä»»åŠ¡ (å¦‚æœæ²¡æœ‰ä¼ å…¥ task_id)
        if not task_id:
            task_id = self.create_task(report_folder, report_name)
            
        self.db.add_log(task_id, 'INFO', 'å¼€å§‹æŠ½å–æµç¨‹')
        
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºrunning
            self.db.update_task_status(task_id, 'running')
            self._check_cancellation(task_id)
            
            # --- V3.2 æ–°å¢ï¼šæ³¨å†Œæ¨¡ç‰ˆæ˜ å°„ ---
            self.db.add_log(task_id, 'INFO', 'æ³¨å†Œæ¨¡ç‰ˆæ˜ å°„...')
            for type_key, template_path in templates.items():
                self._check_cancellation(task_id)
                try:
                    analyzer = TemplateAnalyzer(template_path)
                    # æ˜ç¡®ä¼ å…¥artifact_type, é¿å…æ¨¡ç‰ˆä¸ç¡®å®šæ€§
                    mappings = analyzer.get_template_definitions(type_key)
                    self.db.register_template_mappings(mappings)
                    self.db.add_log(task_id, 'INFO', f'å·²æ³¨å†Œ {type_key} æ¨¡ç‰ˆæ˜ å°„')
                except Exception as e:
                    self.db.add_log(task_id, 'WARNING', f'{type_key} æ¨¡ç‰ˆæ³¨å†Œå¤±è´¥: {str(e)}')
            # -----------------------------

            # 2. ç´¢å¼•å›¾ç‰‡
            self._check_cancellation(task_id)
            self.db.add_log(task_id, 'INFO', 'ç´¢å¼•å›¾ç‰‡...')
            image_stats = self._index_images(task_id, report_folder)
            self.db.add_log(task_id, 'INFO', f'å›¾ç‰‡ç´¢å¼•å®Œæˆ: {image_stats["total"]}å¼ ')
            
            # 3. æŠ½å–é—å€ä¿¡æ¯
            # V3.3 Update: å°è¯•å¤ç”¨å·²å­˜åœ¨çš„Site IDï¼Œå®ç°å¢é‡æ›´æ–°
            self._check_cancellation(task_id)
            existing_site = self.db.get_site_by_report(report_folder)
            
            if 'site' in templates:
                self.db.add_log(task_id, 'INFO', 'æŠ½å–é—å€ä¿¡æ¯...')
                
                if existing_site:
                    # ... (existing logic for updating site by ID) ...
                    site_id = existing_site['id']
                    self.db.add_log(task_id, 'INFO', f'å‘ç°å·²æœ‰é—å€è®°å½• (ID: {site_id})ï¼Œå°†æ‰§è¡Œæ›´æ–°æ¨¡å¼')
                    new_site_id = self._extract_site(task_id, report_folder, templates['site'], existing_site_id=site_id)
                    site_id = new_site_id
                else:
                    # V3.4 Update: è·¨æŠ¥å‘Šåˆå¹¶é€»è¾‘
                    # åœ¨åˆ›å»ºæ–°Siteä¹‹å‰ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨åŒåçš„Site
                    # è¿™éœ€è¦å…ˆæŠ½å–Siteä¿¡æ¯ï¼Œçœ‹çœ‹åå­—æ˜¯å•¥ï¼Œç„¶åå†å†³å®šæ˜¯ Insert è¿˜æ˜¯ Update
                    
                    # 1. é¢„æŠ½å–ä¿¡æ¯ (ä¸æ’å…¥DB)
                    pre_site_data = self._extract_site_data_only(task_id, report_folder, templates['site'])
                    site_name = pre_site_data.get('site_name')
                    
                    found_site = None
                    if site_name:
                        # 2. æŒ‰åç§°æŸ¥æ‰¾ç°æœ‰é—å€
                        found_site = self.db.get_site_by_name(site_name)
                        
                    if found_site:
                        site_id = found_site['id']
                        self.db.add_log(task_id, 'INFO', f'æ ¹æ®åç§° "{site_name}" åŒ¹é…åˆ°å·²æœ‰é—å€ (ID: {site_id})ï¼Œåˆå¹¶æ•°æ®')
                        
                        # 3. æ‰§è¡Œæ›´æ–°
                        # æ›´æ–° task_id å…³è” (å¯é€‰ï¼Œæˆ–è€…è®°å½• log)
                        # æ›´æ–° Site ä¿¡æ¯
                        self.db.update_site(site_id, pre_site_data)
                    else:
                        # 3. æ’å…¥æ–°é—å€
                        site_id = self.db.insert_site(pre_site_data)
                    
                    # æ›´æ–°ä»»åŠ¡çš„site_id
                    self.db.conn.execute(
                        'UPDATE extraction_tasks SET site_id = ? WHERE task_id = ?',
                        (site_id, task_id)
                    )
                    self.db.conn.commit()
                    
                self.db.add_log(task_id, 'INFO', f'é—å€ä¿¡æ¯å¤„ç†å®Œæˆ: site_id={site_id}')
            else:
                # æ²¡é€‰é—å€æ¨¡ç‰ˆ
                if existing_site:
                    site_id = existing_site['id']
                    self.db.add_log(task_id, 'INFO', f'å¤ç”¨å·²æœ‰é—å€ ID: {site_id}')
                else:
                    # å°è¯•æ ¹æ®æŠ¥å‘ŠåçŒœæµ‹é—å€åå¹¶æŸ¥æ‰¾ (ç®€å•é€»è¾‘)
                    report_name = os.path.basename(report_folder)
                    # å‡è®¾æŠ¥å‘ŠååŒ…å«é—å€å
                    found_site = self.db.get_site_by_name(report_name) 
                    if found_site:
                        site_id = found_site['id']
                        self.db.add_log(task_id, 'INFO', f'æ ¹æ®æŠ¥å‘ŠåçŒœæµ‹åŒ¹é…åˆ°é—å€ ID: {site_id}')
                    else:
                        site_id = None
            
            # 4. æŠ½å–æ—¶æœŸä¿¡æ¯
            period_count = 0
            if 'period' in templates and site_id:
                self._check_cancellation(task_id)
                self.db.add_log(task_id, 'INFO', 'æŠ½å–æ—¶æœŸä¿¡æ¯...')
                period_count = self._extract_periods(task_id, site_id, report_folder, templates['period'])
                self.db.add_log(task_id, 'INFO', f'æ—¶æœŸä¿¡æ¯æŠ½å–å®Œæˆ: {period_count}ä¸ª')
            
            # 5. æŠ½å–é™¶å™¨ä¿¡æ¯
            pottery_count = 0
            if 'pottery' in templates:
                self._check_cancellation(task_id)
                self.db.add_log(task_id, 'INFO', 'æŠ½å–é™¶å™¨ä¿¡æ¯...')
                pottery_count = self._extract_artifacts(
                    task_id, site_id, report_folder, templates['pottery'], 'pottery'
                )
                self.db.add_log(task_id, 'INFO', f'é™¶å™¨ä¿¡æ¯æŠ½å–å®Œæˆ: {pottery_count}ä»¶')
            
            # 6. æŠ½å–ç‰å™¨ä¿¡æ¯
            jade_count = 0
            if 'jade' in templates:
                self._check_cancellation(task_id)
                self.db.add_log(task_id, 'INFO', 'æŠ½å–ç‰å™¨ä¿¡æ¯...')
                jade_count = self._extract_artifacts(
                    task_id, site_id, report_folder, templates['jade'], 'jade'
                )
                self.db.add_log(task_id, 'INFO', f'ç‰å™¨ä¿¡æ¯æŠ½å–å®Œæˆ: {jade_count}ä»¶')
            
            # 7. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.db.update_task_statistics(task_id, {
                'total_pottery': pottery_count,
                'total_jade': jade_count,
                'total_periods': period_count if 'period_count' in locals() else 0,
                'total_images': image_stats['total']
            })
            
            # 8. å®Œæˆä»»åŠ¡
            self.db.update_task_status(task_id, 'completed')
            self.db.add_log(task_id, 'INFO', 'æŠ½å–æµç¨‹å®Œæˆ')
            
            return task_id
            
        except Exception as e:
            self.db.add_log(task_id, 'ERROR', f'æŠ½å–å¤±è´¥: {str(e)}')
            self.db.update_task_status(task_id, 'failed')
            # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
            import traceback
            error_detail = traceback.format_exc()
            self.db.add_log(task_id, 'ERROR', f'é”™è¯¯è¯¦æƒ…: {error_detail[:500]}')
            raise
    
    def create_task(self, report_folder: str, report_name: Optional[str] = None) -> str:
        """åˆ›å»ºæŠ½å–ä»»åŠ¡"""
        import random
        # æ·»åŠ éšæœºåç¼€ä»¥æ”¯æŒå¹¶å‘ä»»åŠ¡åœ¨åŒä¸€ç§’å†…åˆ›å»º
        random_suffix = f"{random.randint(1000, 9999)}"
        task_id = f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random_suffix}"
        
        if not report_name:
            report_name = os.path.basename(report_folder)
        
        # æŸ¥æ‰¾æŠ¥å‘Šæ–‡ä»¶
        markdown_path = self._find_file(report_folder, 'full.md')
        layout_json_path = self._find_file(report_folder, 'layout.json')
        content_list_json_path = self._find_file(report_folder, '*_content_list.json')
        images_folder_path = os.path.join(report_folder, 'images')
        
        task_data = {
            'task_id': task_id,
            'report_name': report_name,
            'report_folder_path': report_folder,
            'markdown_path': markdown_path,
            'layout_json_path': layout_json_path,
            'content_list_json_path': content_list_json_path,
            'images_folder_path': images_folder_path if os.path.exists(images_folder_path) else None
        }
        
        self.db.create_task(task_data)
        return task_id
    
    def _find_file(self, folder: str, pattern: str) -> Optional[str]:
        """æŸ¥æ‰¾æ–‡ä»¶"""
        if '*' in pattern:
            # é€šé…ç¬¦åŒ¹é…
            import glob
            files = glob.glob(os.path.join(folder, pattern))
            return files[0] if files else None
        else:
            # ç²¾ç¡®åŒ¹é…
            file_path = os.path.join(folder, pattern)
            return file_path if os.path.exists(file_path) else None
    
    def _index_images(self, task_id: str, report_folder: str) -> Dict:
        """ç´¢å¼•å›¾ç‰‡"""
        img_manager = ImageManager(report_folder)
        images_data = img_manager.index_all_images()
        
        # æ’å…¥æ•°æ®åº“ï¼ˆä½¿ç”¨INSERT OR IGNOREé¿å…é‡å¤ï¼‰
        for img_data in images_data:
            img_data['task_id'] = task_id
            try:
                self.db.insert_image(img_data)
            except Exception as e:
                # å¦‚æœå›¾ç‰‡å·²å­˜åœ¨ï¼ˆè¿åUNIQUEçº¦æŸï¼‰ï¼Œè·³è¿‡
                if 'UNIQUE constraint failed' in str(e):
                    continue
                else:
                    raise
        
        return img_manager.get_statistics()
    
    def _expand_code_with_llm(self, code: str) -> List[str]:
        """
        ä½¿ç”¨LLMæ™ºèƒ½è§£æå¤æ‚çš„æ–‡ç‰©ç¼–å·èŒƒå›´
        """
        try:
            # æ„é€ ä¸“é—¨çš„Prompt
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è€ƒå¤æ•°æ®å¤„ç†åŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹åŒ…å«èŒƒå›´æˆ–åˆ—è¡¨çš„æ–‡ç‰©ç¼–å·å­—ç¬¦ä¸²ï¼Œè§£æå±•å¼€ä¸ºæ ‡å‡†çš„ç‹¬ç«‹æ–‡ç‰©ç¼–å·åˆ—è¡¨ã€‚

ç¤ºä¾‹ 1:
è¾“å…¥: "M7:63-1~3"
è¾“å‡º: ["M7:63-1", "M7:63-2", "M7:63-3"]

ç¤ºä¾‹ 2:
è¾“å…¥: "M7:1ã€2ã€5"
è¾“å‡º: ["M7:1", "M7:2", "M7:5"]

ç¤ºä¾‹ 3:
è¾“å…¥: "M7:63-1~63-3"
è¾“å‡º: ["M7:63-1", "M7:63-2", "M7:63-3"]

å¾…å¤„ç†è¾“å…¥: "{code}"

è¯·ç›´æ¥è¿”å›JSONå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¸è¦åŒ…å«Markdownæ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰æˆ–å…¶ä»–è§£é‡Šæ€§æ–‡å­—ã€‚
"""
            # è°ƒç”¨LLMï¼ˆä½¿ç”¨è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—ç¡®å®šçš„ç»“æœï¼‰
            config = self.llm_config.copy()
            if 'llm' in config:
                config['llm']['temperature'] = 0.1
            
            response = call_llm_api(prompt, config)
            result = extract_json_from_response(response)
            
            if isinstance(result, list):
                # è¿‡æ»¤éå­—ç¬¦ä¸²é¡¹
                return [str(item) for item in result if item]
            return []
            
        except Exception as e:
            print(f"LLM expansion failed for {code}: {e}")
            return []

    def _expand_artifact_ranges(self, artifacts: List[Dict]) -> List[Dict]:
        """
        æ‰©å±•åŒ…å«èŒƒå›´çš„æ–‡ç‰©ç¼–å·ï¼Œé‡‡ç”¨ "è§„åˆ™ä¼˜å…ˆ + LLMå…œåº•" çš„ç­–ç•¥
        """
        import re
        expanded = []
        
        for artifact in artifacts:
            # V3.10 Fix: Handle artifact_code being None
            code = artifact.get('artifact_code')
            if code is None:
                code = ''
            else:
                code = str(code).strip()
                
            is_expanded = False
            
            # 1. è§„åˆ™å±‚ï¼šå°è¯•å¤„ç†æ ‡å‡†çš„ '~' èŒƒå›´
            if '~' in code:
                try:
                    parts = code.split('~')
                    if len(parts) == 2:
                        start_full = parts[0].strip()
                        end_full = parts[1].strip()
                        
                        # è§£æèµ·å§‹ç¼–å·
                        start_match = re.search(r'^(.*?)(\d+)$', start_full)
                        if start_match:
                            prefix = start_match.group(1)
                            start_num = int(start_match.group(2))
                            
                            # è§£æç»“æŸç¼–å·
                            end_match = re.search(r'(\d+)$', end_full)
                            if end_match:
                                end_num = int(end_match.group(1))
                                
                                # éªŒè¯èŒƒå›´åˆç†æ€§
                                if start_num < end_num and (end_num - start_num) < 100:
                                    for i in range(start_num, end_num + 1):
                                        new_artifact = artifact.copy()
                                        new_artifact['artifact_code'] = f"{prefix}{i}"
                                        expanded.append(new_artifact)
                                    is_expanded = True
                except Exception:
                    pass # è§„åˆ™è§£æå¤±è´¥ï¼Œç•™ç»™LLMå¤„ç†
            
            # 2. å…œåº•å±‚ï¼šå¦‚æœè§„åˆ™æœªå¤„ç†ï¼Œä¸”çœ‹èµ·æ¥åƒå¤æ‚åˆ—è¡¨ï¼ˆåŒ…å«åˆ†éš”ç¬¦ï¼‰ï¼Œåˆ™è°ƒç”¨LLM
            # æ£€æŸ¥å¸¸è§åˆ†éš”ç¬¦ï¼šã€ , å’Œ è‡³
            if not is_expanded:
                complex_indicators = ['ã€', ',', 'ï¼Œ', 'å’Œ', 'è‡³', '&']
                # å¦‚æœåŒ…å«ä¸Šè¿°ç¬¦å·ï¼Œæˆ–è€…åŒ…å« ~ ä½†ä¸Šé¢æ²¡å¤„ç†æˆåŠŸ
                if any(char in code for char in complex_indicators) or ('~' in code and not is_expanded):
                    
                    print(f"ğŸ” æ£€æµ‹åˆ°å¤æ‚ç¼–å· '{code}'ï¼Œæ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½å±•å¼€...")
                    expanded_codes = self._expand_code_with_llm(code)
                    
                    if expanded_codes:
                        print(f"   -> LLMå±•å¼€ç»“æœ: {expanded_codes}")
                        for new_code in expanded_codes:
                            new_artifact = artifact.copy()
                            new_artifact['artifact_code'] = new_code
                            expanded.append(new_artifact)
                        is_expanded = True
            
            # 3. å¦‚æœéƒ½æ²¡å¤„ç†ï¼Œä¿ç•™åŸæ ·
            if not is_expanded:
                expanded.append(artifact)
            
        return expanded

    def _generate_triples(self, data: Dict, artifact_type: str, artifact_id: int, task_id: str):
        """ç”Ÿæˆå¹¶æ’å…¥è¯­ä¹‰ä¸‰å…ƒç»„"""
        try:
            # 1. è·å–æ¨¡ç‰ˆæ˜ å°„ä¿¡æ¯ (ID & Property)
            cursor = self.db.conn.cursor()
            # V3.6 Fix: åŒæ—¶æŸ¥è¯¢ä¸­æ–‡å’Œè‹±æ–‡å­—æ®µåï¼Œä»¥æ”¯æŒLLMè¿”å›ä»»ä¸€ç§æ ¼å¼
            cursor.execute(
                'SELECT field_name_cn, field_name_en, id, cidoc_property FROM sys_template_mappings WHERE artifact_type = ?',
                (artifact_type,)
            )
            mappings = cursor.fetchall() # [(name_cn, name_en, id, prop), ...]
            
            import re
            def clean_string(s): return re.sub(r'\s+', '', str(s)).lower()
            
            # æ„å»ºæŸ¥æ‰¾è¡¨: clean_name -> (id, prop)
            map_lookup = {}
            for name_cn, name_en, mid, prop in mappings:
                if name_cn:
                    map_lookup[clean_string(name_cn)] = (mid, prop)
                if name_en:
                    map_lookup[clean_string(name_en)] = (mid, prop)
                
            triples = []
            for key, value in data.items():
                if not value: continue
                
                # å°è¯•åŒ¹é…
                clean_key = clean_string(key)
                match = map_lookup.get(clean_key)
                
                if match:
                    mid, prop = match
                    if prop: # åªæœ‰å®šä¹‰äº†Propertyçš„å­—æ®µæ‰ç”Ÿæˆä¸‰å…ƒç»„
                        triples.append({
                            'artifact_type': artifact_type,
                            'artifact_id': artifact_id,
                            'mapping_id': mid,
                            'predicate': prop,
                            'object_value': str(value),
                            'confidence': data.get('extraction_confidence', 1.0)
                        })
                        
            if triples:
                self.db.insert_fact_triples(triples)
        except Exception as e:
            self.db.add_log(task_id, 'WARNING', f'ç”Ÿæˆä¸‰å…ƒç»„å¤±è´¥: {str(e)}')

    def _extract_site_data_only(self, task_id: str, report_folder: str, template_path: str) -> Dict:
        """
        ä»…æŠ½å–é—å€æ•°æ®ï¼Œä¸æ’å…¥æ•°æ®åº“
        ç”¨äºé¢„æ£€æŸ¥é—å€åç§°
        """
        # è¯»å–æŠ¥å‘Šæ–‡æœ¬
        markdown_path = os.path.join(report_folder, 'full.md')
        if not os.path.exists(markdown_path):
            raise FileNotFoundError(f"æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {markdown_path}")
        
        with open(markdown_path, 'r', encoding='utf-8') as f:
            full_text = f.read()
        
        # å–å‰5000å­—
        site_text = full_text[:5000]
        
        # ç”Ÿæˆæç¤ºè¯
        prompt = self.prompt_generator.generate_prompt(
            'site', template_path, site_text, {'report_name': task_id}
        )
        
        # è°ƒç”¨LLM
        response = call_llm_api(prompt, self.llm_config)
        site_data = extract_json_from_response(response)
        
        # è¡¥å……åŸºç¡€å­—æ®µ
        site_data['task_id'] = task_id
        site_data['source_text_blocks'] = json.dumps([0])
        site_data['extraction_confidence'] = 0.8
        
        # ä¿å­˜åŸå§‹æ•°æ®åˆ° raw_attributes
        system_fields = ['task_id', 'source_text_blocks', 'extraction_confidence']
        raw_dict = {k: v for k, v in site_data.items() if k not in system_fields}
        site_data['raw_attributes'] = json.dumps(raw_dict, ensure_ascii=False)
        
        # ç¡®ä¿ site_name
        if 'site_name' not in site_data or not site_data['site_name']:
            for k in ['é—å€åç§°', 'åç§°', 'Name', 'Site Name']:
                if site_data.get(k):
                    site_data['site_name'] = site_data[k]
                    break
            if 'site_name' not in site_data or not site_data['site_name']:
                # V3.3 Fix: ä½¿ç”¨æŠ¥å‘Šåç§°ä½œä¸ºå…œåº•
                task_info = self.db.get_task(task_id)
                report_name = task_info.get('report_name', 'Unknown Site') if task_info else 'Unknown Site'
                site_data['site_name'] = report_name
                # åªæ˜¯é¢„æŠ½å–ï¼Œä½†ä¸ºäº†åç»­insert_siteä¸æŠ¥é”™ï¼Œå¿…é¡»èµ‹å€¼
                pass
                
        return site_data

    def _extract_site(self, task_id: str, report_folder: str, template_path: str, existing_site_id: int = None) -> int:
        """æŠ½å–é—å€ä¿¡æ¯"""
        # è¯»å–æŠ¥å‘Šæ–‡æœ¬
        markdown_path = os.path.join(report_folder, 'full.md')
        if not os.path.exists(markdown_path):
            raise FileNotFoundError(f"æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {markdown_path}")
        
        with open(markdown_path, 'r', encoding='utf-8') as f:
            full_text = f.read()
        
        # å–å‰5000å­—ä½œä¸ºé—å€ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æŠ¥å‘Šå¼€å¤´ï¼‰
        site_text = full_text[:5000]
        
        # ç”Ÿæˆæç¤ºè¯
        prompt = self.prompt_generator.generate_prompt(
            'site', template_path, site_text, {'report_name': task_id}
        )
        
        # è°ƒç”¨LLM
        response = call_llm_api(prompt, self.llm_config)
        site_data = extract_json_from_response(response)
        
        # æ’å…¥æ•°æ®åº“
        site_data['task_id'] = task_id
        site_data['source_text_blocks'] = json.dumps([0])  # æ–‡æœ¬å—ç´¢å¼•
        site_data['extraction_confidence'] = 0.8
        
        # ä¿å­˜åŸå§‹æ•°æ®åˆ° raw_attributes (æ’é™¤ç³»ç»Ÿå­—æ®µ)
        # è¿™ç¡®ä¿äº†å³ä½¿æŸäº›å­—æ®µå› æ˜ å°„é—®é¢˜è¢«è¿‡æ»¤ï¼ŒåŸå§‹æ•°æ®ä»ç„¶ä¿ç•™
        system_fields = ['task_id', 'source_text_blocks', 'extraction_confidence']
        raw_dict = {k: v for k, v in site_data.items() if k not in system_fields}
        site_data['raw_attributes'] = json.dumps(raw_dict, ensure_ascii=False)
        
        # V3.3 Fix: ç¡®ä¿ site_name å­˜åœ¨
        if 'site_name' not in site_data or not site_data['site_name']:
            # å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„é”®å
            for k in ['é—å€åç§°', 'åç§°', 'Name', 'Site Name']:
                if site_data.get(k):
                    site_data['site_name'] = site_data[k]
                    break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨æŠ¥å‘Šåç§°ä½œä¸ºå…œåº•
            if 'site_name' not in site_data or not site_data['site_name']:
                task_info = self.db.get_task(task_id)
                report_name = task_info.get('report_name', 'Unknown Site') if task_info else 'Unknown Site'
                site_data['site_name'] = report_name
                self.db.add_log(task_id, 'WARNING', f'æœªæå–åˆ°é—å€åç§°ï¼Œä½¿ç”¨æŠ¥å‘Šåç§° "{report_name}" ä»£æ›¿')
        
        print(f"DEBUG: site_name before insert: {site_data.get('site_name')}") # Debug print

        if existing_site_id:
            # æ›´æ–°æ¨¡å¼
            self.db.update_site(existing_site_id, site_data)
            site_id = existing_site_id
        else:
            site_id = self.db.insert_site(site_data)
        
        # V3.5: ç”Ÿæˆå¹¶æ’å…¥é—å€çš„è¯­ä¹‰ä¸‰å…ƒç»„
        self._generate_triples(site_data, 'site', site_id, task_id)

        # V3.9: å¤„ç†é—å€ç»“æ„ (Structures)
        structures = site_data.get('structures', [])
        if structures and isinstance(structures, list):
            self.db.add_log(task_id, 'INFO', f'å‘ç° {len(structures)} ä¸ªé—å€ç»“æ„å•å…ƒï¼Œæ­£åœ¨å¤„ç†...')
            
            # ç¬¬ä¸€è½®ï¼šæ’å…¥æ‰€æœ‰ç»“æ„ï¼Œå»ºç«‹åç§°æ˜ å°„
            structure_name_map = {} # name -> id
            
            for struct in structures:
                if not isinstance(struct, dict): continue
                struct_name = struct.get('structure_name')
                if not struct_name: continue
                
                # å‡†å¤‡æ•°æ®
                struct_data = {
                    'site_id': site_id,
                    'structure_name': struct_name,
                    'structure_type': struct.get('structure_type'),
                    'description': struct.get('description'),
                    'source_text_blocks': json.dumps([0])
                }
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                existing_struct = self.db.get_structure_by_name(site_id, struct_name)
                if existing_struct:
                    # æ›´æ–°
                    struct_id = existing_struct['id']
                    self.db.update_structure(struct_id, struct_data)
                else:
                    # æ’å…¥
                    struct_id = self.db.insert_structure(struct_data)
                
                structure_name_map[struct_name] = struct_id
            
            # ç¬¬äºŒè½®ï¼šæ›´æ–°çˆ¶å­å…³ç³»
            for struct in structures:
                struct_name = struct.get('structure_name')
                parent_name = struct.get('parent_structure_name')
                
                if struct_name and parent_name and struct_name in structure_name_map:
                    parent_id = structure_name_map.get(parent_name)
                    if parent_id:
                        struct_id = structure_name_map[struct_name]
                        # æ›´æ–° parent_id
                        self.db.conn.execute(
                            'UPDATE site_structures SET parent_id = ? WHERE id = ?',
                            (parent_id, struct_id)
                        )
            self.db.conn.commit()
            self.db.add_log(task_id, 'INFO', f'é—å€ç»“æ„å¤„ç†å®Œæˆ')
        
        # æ›´æ–°ä»»åŠ¡çš„site_id
        self.db.conn.execute(
            'UPDATE extraction_tasks SET site_id = ? WHERE task_id = ?',
            (site_id, task_id)
        )
        self.db.conn.commit()
        
        return site_id
    
    def _extract_periods(self, task_id: str, site_id: int, 
                        report_folder: str, template_path: str) -> int:
        """æŠ½å–æ—¶æœŸä¿¡æ¯"""
        # è¯»å–æŠ¥å‘Šæ–‡æœ¬
        markdown_path = os.path.join(report_folder, 'full.md')
        with open(markdown_path, 'r', encoding='utf-8') as f:
            full_text = f.read()
        
        # æŸ¥æ‰¾æ—¶æœŸç›¸å…³ç« èŠ‚ï¼ˆé€šå¸¸åœ¨æŠ¥å‘Šä¸­éƒ¨ï¼‰
        period_text = full_text[5000:15000]  # ç®€åŒ–å¤„ç†
        
        # ç”Ÿæˆæç¤ºè¯
        site_info = self.db.get_site_by_task(task_id)
        context = {'site_name': site_info.get('site_name', '')} if site_info else {}
        
        prompt = self.prompt_generator.generate_prompt(
            'period', template_path, period_text, context
        )
        
        # è°ƒç”¨LLM
        response = call_llm_api(prompt, self.llm_config)
        periods_data = extract_json_from_response(response)
        
        # ç¡®ä¿æ˜¯åˆ—è¡¨
        if isinstance(periods_data, dict):
            periods_data = [periods_data]
        
        # æ’å…¥æ•°æ®åº“
        for period_data in periods_data:
            period_data['task_id'] = task_id
            period_data['site_id'] = site_id
            period_data['source_text_blocks'] = json.dumps([1])
            period_data['extraction_confidence'] = 0.8
            period_id = self.db.insert_period(period_data)
            
            # V3.5: ç”Ÿæˆå¹¶æ’å…¥æ—¶æœŸçš„è¯­ä¹‰ä¸‰å…ƒç»„
            if period_id:
                self._generate_triples(period_data, 'period', period_id, task_id)
        
        return len(periods_data)
    
    def _split_large_text(self, text: str, chunk_size: int = 2000, overlap: int = 200) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬æ™ºèƒ½åˆ‡åˆ†ä¸ºé‡å çš„ç‰‡æ®µï¼Œä¼˜å…ˆåœ¨æ¢è¡Œç¬¦å¤„åˆ‡åˆ†
        """
        if len(text) <= chunk_size:
            return [text]
            
        chunks = []
        start = 0
        
        while start < len(text):
            # é¢„è®¾ç»“æŸä½ç½®
            end = start + chunk_size
            
            # å¦‚æœè¶…å‡ºæ€»é•¿åº¦ï¼Œå°±åˆ°æœ€å
            if end >= len(text):
                chunks.append(text[start:])
                break
                
            # åœ¨ chunk_size èŒƒå›´å†…å¯»æ‰¾æœ€è¿‘çš„æ¢è¡Œç¬¦ï¼Œé¿å…åˆ‡æ–­å¥å­
            # æˆ‘ä»¬åœ¨ end é™„è¿‘å‘å‰æ‰¾æ¢è¡Œç¬¦
            # æœç´¢èŒƒå›´: [end - overlap, end]
            search_start = max(start, end - overlap)
            last_newline = text.rfind('\n', search_start, end)
            
            if last_newline != -1:
                # æ‰¾åˆ°äº†æ¢è¡Œç¬¦ï¼Œåœ¨æ­¤å¤„åˆ‡åˆ†
                actual_end = last_newline + 1 # åŒ…å«æ¢è¡Œç¬¦
            else:
                # æ²¡æ‰¾åˆ°æ¢è¡Œç¬¦ï¼Œå°è¯•æ‰¾å¥å·
                last_period = text.rfind('ã€‚', search_start, end)
                if last_period != -1:
                    actual_end = last_period + 1
                else:
                    # å®åœ¨æ‰¾ä¸åˆ°åˆ†éš”ç¬¦ï¼Œå°±ç¡¬åˆ‡
                    actual_end = end
            
            chunks.append(text[start:actual_end])
            
            # ä¸‹ä¸€æ®µçš„å¼€å§‹ä½ç½® = å½“å‰ç»“æŸä½ç½® - é‡å é‡ (ä¸ºäº†ä¸Šä¸‹æ–‡è¿ç»­æ€§)
            # å¦‚æœæ˜¯æŒ‰æ¢è¡Œç¬¦åˆ‡çš„ï¼Œå…¶å®å¯ä»¥ä¸é‡å ï¼Œä½†ä¸ºäº†ä¿é™©èµ·è§ï¼Œå¦‚æœæ˜¯ç¡¬åˆ‡çš„éœ€è¦é‡å 
            # è¿™é‡Œç®€å•å¤„ç†ï¼šç›´æ¥ä» actual_end å¼€å§‹ï¼Œä¸åšé¢å¤–é‡å ï¼Œ
            # å› ä¸º ArtifactMerger ä¼šå¤„ç†è·¨ç‰‡æ®µçš„å®ä½“ï¼Œ
            # ä½†ä¸ºäº†é˜²æ­¢ä¸€ä¸ªå®ä½“æè¿°æ­£å¥½è¢«åˆ‡æ–­ï¼Œæˆ‘ä»¬è¿˜æ˜¯ç¨å¾®å›é€€ä¸€ç‚¹ç‚¹ï¼Œæˆ–è€…ä¾èµ–ArtifactMerger
            # è€ƒè™‘åˆ°æˆ‘ä»¬çš„mergeræ˜¯åŸºäº artifact_code çš„ï¼Œå¦‚æœcodeè¢«åˆ‡æ–­äº†å°±éº»çƒ¦äº†ã€‚
            # æ‰€ä»¥ä¿ç•™ overlap æ˜¯å®‰å…¨çš„ã€‚
            start = max(start + 1, actual_end - overlap) # ç¡®ä¿è‡³å°‘å‰è¿›1ä¸ªå­—ç¬¦
            
        return chunks

    def _extract_artifacts(self, task_id: str, site_id: Optional[int],
                          report_folder: str, template_path: str,
                          artifact_type: str) -> int:
        """
        æŠ½å–æ–‡ç‰©ä¿¡æ¯
        
        Args:
            task_id: ä»»åŠ¡ID
            site_id: é—å€ID
            report_folder: æŠ¥å‘Šæ–‡ä»¶å¤¹
            template_path: æ¨¡æ¿è·¯å¾„
            artifact_type: æ–‡ç‰©ç±»å‹ (pottery/jade)
        
        Returns:
            æŠ½å–çš„æ–‡ç‰©æ•°é‡
        """
        # è¯»å–æŠ¥å‘Šæ–‡æœ¬
        markdown_path = os.path.join(report_folder, 'full.md')
        with open(markdown_path, 'r', encoding='utf-8') as f:
            full_text = f.read()
        
        # æŒ‰å¢“è‘¬åˆ†å—
        tomb_dict = split_by_tomb(full_text)
        
        if not tomb_dict:
            self.db.add_log(task_id, 'WARNING', f'æœªæ‰¾åˆ°å¢“è‘¬åˆ†å—ï¼Œä½¿ç”¨æ•´ä½“æ–‡æœ¬')
            tomb_blocks = [('å…¨æ–‡', full_text)]
        else:
            # å°†å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨ [(tomb_name, tomb_text), ...]
            tomb_blocks = list(tomb_dict.items())
        
        self.db.add_log(task_id, 'INFO', f'æ–‡æœ¬åˆ†ä¸º{len(tomb_blocks)}ä¸ªå¢“è‘¬å—')
        
        # è·å–ç«™ç‚¹ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡
        site_info = self.db.get_site_by_task(task_id) if site_id else {}
        
        # é€å—æŠ½å–
        all_artifacts = []
        for i, tomb_block in enumerate(tomb_blocks):
            self._check_cancellation(task_id)
            tomb_name, tomb_text = tomb_block
            self.db.add_log(task_id, 'INFO', f'å¤„ç† {tomb_name} ({i+1}/{len(tomb_blocks)})')
            
            # V3.3: æ™ºèƒ½åˆ‡åˆ†é•¿æ–‡æœ¬
            # å¦‚æœæ–‡æœ¬è¿‡é•¿(>3000å­—ç¬¦)ï¼Œåˆ‡åˆ†ä¸ºç‰‡æ®µåˆ†åˆ«æŠ½å–ï¼Œé˜²æ­¢LLMå“åº”æˆªæ–­
            text_chunks = self._split_large_text(tomb_text, chunk_size=3000, overlap=300)
            
            if len(text_chunks) > 1:
                self.db.add_log(task_id, 'INFO', f'æ–‡æœ¬è¿‡é•¿ï¼Œå·²åˆ‡åˆ†ä¸º {len(text_chunks)} ä¸ªç‰‡æ®µè¿›è¡ŒæŠ½å–')
            
            for chunk_idx, chunk_text in enumerate(text_chunks):
                self._check_cancellation(task_id)
                if len(text_chunks) > 1:
                    self.db.add_log(task_id, 'INFO', f'  -> æ­£åœ¨æŠ½å–ç‰‡æ®µ {chunk_idx+1}/{len(text_chunks)}...')
                
                # ç”Ÿæˆæç¤ºè¯
                context = {
                    'site_name': site_info.get('site_name', '') if site_info else '',
                    'tomb_name': tomb_name
                }
                
                # å¦‚æœæ˜¯åˆ‡åˆ†ç‰‡æ®µï¼Œæœ€å¥½åœ¨prompté‡Œæç¤ºä¸€ä¸‹ï¼ˆå¯é€‰ï¼Œç›®å‰promptæ¨¡æ¿æ¯”è¾ƒé€šç”¨ï¼Œå¯èƒ½ä¸éœ€è¦ï¼‰
                prompt = self.prompt_generator.generate_prompt(
                    artifact_type, template_path, chunk_text, context
                )
                
                try:
                    # è°ƒç”¨LLM
                    response = call_llm_api(prompt, self.llm_config)
                    artifacts = extract_json_from_response(response)
                    
                    # ç¡®ä¿æ˜¯åˆ—è¡¨
                    if isinstance(artifacts, dict):
                        artifacts = [artifacts]
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    for artifact in artifacts:
                        artifact['task_id'] = task_id
                        artifact['site_id'] = site_id
                        # è®°å½•æºæ–‡æœ¬å—ç´¢å¼•ï¼šè¿™é‡Œå­˜çš„æ˜¯ tomb_idxï¼Œä¸æ˜¯ chunk_idx
                        artifact['source_text_blocks'] = json.dumps([i]) 
                        artifact['extraction_confidence'] = 0.8
                        artifact['found_in_tomb'] = tomb_name
                    
                    all_artifacts.extend(artifacts)
                    self.db.add_log(task_id, 'INFO', f'{tomb_name} (ç‰‡æ®µ{chunk_idx+1}) æŠ½å–åˆ° {len(artifacts)} ä»¶')
                    
                except Exception as e:
                    error_msg = f'{tomb_name} (ç‰‡æ®µ{chunk_idx+1}) æŠ½å–å¤±è´¥: {str(e)}'
                    self.db.add_log(task_id, 'ERROR', error_msg)
                    
                    # --- è¡¥æ•‘æœºåˆ¶ï¼šä¿å­˜å¤±è´¥çš„åŸå§‹å“åº” ---
                    if 'response' in locals() and response:
                        try:
                            log_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'logs', 'failed_responses')
                            os.makedirs(log_dir, exist_ok=True)
                            
                            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                            # æ–‡ä»¶ååŒ…å« task_id ä»¥ä¾¿å…³è”
                            filename = f"failed_{task_id}_{timestamp}_{i}_{chunk_idx}.txt"
                            filepath = os.path.join(log_dir, filename)
                            
                            with open(filepath, 'w', encoding='utf-8') as f:
                                f.write(f"Task ID: {task_id}\n")
                                f.write(f"Artifact Type: {artifact_type}\n")
                                f.write(f"Context: {tomb_name} (Chunk {chunk_idx+1})\n")
                                f.write(f"Error: {str(e)}\n")
                                f.write("-" * 50 + "\n")
                                f.write(response)
                                
                            self.db.add_log(task_id, 'WARNING', f'å·²ä¿å­˜å¤±è´¥çš„å“åº”ç‰‡æ®µè‡³: {filename}ï¼Œå¯åœ¨ä»»åŠ¡ç®¡ç†ä¸­æŸ¥çœ‹å¹¶æ¢å¤')
                        except Exception as save_err:
                            print(f"ä¿å­˜å¤±è´¥å“åº”æ—¶å‡ºé”™: {save_err}")
                    # -----------------------------------
                    
                    continue
        
        
        # V3.3: æ‰©å±•ç¼–å·èŒƒå›´ (å¦‚ M7:63-1~26)
        self.db.add_log(task_id, 'INFO', f'æ£€æŸ¥å¹¶æ‰©å±•æ–‡ç‰©ç¼–å·èŒƒå›´...')
        all_artifacts = self._expand_artifact_ranges(all_artifacts)

        # V3.5: æ¸…æ´—å¢“è‘¬åç§° (Tomb Name Normalization)
        # å¿…é¡»åœ¨åˆå¹¶å‰åšï¼Œä»¥ä¾¿æ­£ç¡®å½’ç±»
        import re
        for artifact in all_artifacts:
            # 1. å°è¯•ä» artifact_code æ¨æ–­ (å¦‚ "M12:1" -> "M12")
            code = artifact.get('artifact_code')
            if code is None:
                code = ''
            else:
                code = str(code).strip()
                
            tomb_val = artifact.get('found_in_tomb', '')
            
            inferred_tomb = None
            if ':' in code:
                parts = code.split(':')
                if parts[0].upper().startswith('M'):
                    inferred_tomb = parts[0].upper()
            
            # 2. å¦‚æœ artifact_code æ¨æ–­å‡ºäº†æœ‰æ•ˆçš„ M å·ï¼Œä¼˜å…ˆä½¿ç”¨
            if inferred_tomb:
                artifact['found_in_tomb'] = inferred_tomb
            else:
                # 3. å¦åˆ™å°è¯•æ¸…æ´—ç°æœ‰çš„ found_in_tomb
                if not tomb_val or str(tomb_val).lower() in ['å…¨æ–‡', 'unknown', 'none', ''] or 'å·å¢“' in str(tomb_val):
                    val_str = str(tomb_val) if tomb_val is not None else ''
                    
                    # å°è¯•åŒ¹é… "å…­å·å¢“" -> "M6"
                    cn_num_map = {'ä¸€':1, 'äºŒ':2, 'ä¸‰':3, 'å››':4, 'äº”':5, 'å…­':6, 'ä¸ƒ':7, 'å…«':8, 'ä¹':9, 'å':10}
                    match = re.search(r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)å·å¢“', val_str)
                    if match:
                        num_str = match.group(1)
                        # ç®€å•è½¬æ¢ (ä»…æ”¯æŒ1-10ï¼Œå¤æ‚çš„æš‚ç•¥)
                        num = cn_num_map.get(num_str)
                        if num:
                            artifact['found_in_tomb'] = f"M{num}"
                    else:
                         # åŒ¹é… "6å·å¢“" -> "M6"
                        match_digit = re.search(r'(\d+)å·å¢“', val_str)
                        if match_digit:
                             artifact['found_in_tomb'] = f"M{match_digit.group(1)}"

        # åˆå¹¶åŒä¸€æ–‡ç‰©çš„ä¿¡æ¯
        self.db.add_log(task_id, 'INFO', f'åˆå¹¶æ–‡ç‰©ä¿¡æ¯...')
        merged_artifacts = self.artifact_merger.merge_artifacts(all_artifacts)
        self.db.add_log(task_id, 'INFO', 
                       f'åˆå¹¶å®Œæˆ: {len(all_artifacts)} -> {len(merged_artifacts)}')
        
        # å‡†å¤‡CIDOCå…ƒæ•°æ®
        analyzer = TemplateAnalyzer(template_path)
        field_metadata = analyzer.get_field_metadata()
        
        # å­—æ®µæ˜ å°„ï¼šä¸­æ–‡ -> è‹±æ–‡ï¼Œå¹¶æ·»åŠ Raw/CIDOCæ•°æ®
        self.db.add_log(task_id, 'INFO', f'æ˜ å°„å­—æ®µåå¹¶ç”ŸæˆCIDOCæ•°æ®...')
        field_mapper = FieldMapper(template_path)
        
        mapped_artifacts = []
        
        for artifact in merged_artifacts:
            # 1. ç”Ÿæˆ Raw Attributes (JSON)
            # æ’é™¤ç³»ç»Ÿç”Ÿæˆçš„å…ƒæ•°æ®å­—æ®µï¼Œåªä¿ç•™æŠ½å–ç›¸å…³çš„
            system_fields = ['task_id', 'site_id', 'source_text_blocks', 'extraction_confidence', 'found_in_tomb']
            raw_dict = {k: v for k, v in artifact.items() if k not in system_fields}
            raw_data = json.dumps(raw_dict, ensure_ascii=False)
            
            # 2. ç”Ÿæˆ CIDOC Attributes (JSON) 
            # V3.6: ä¸‰å…ƒç»„ç”Ÿæˆå·²ç§»è‡³ _generate_triples å¹¶æ”¹ä¸ºæ’å…¥åæ‰§è¡Œï¼Œ
            # ä½† cidoc_attributes ä»ç„¶ä¿ç•™ä»¥ä¾¿å…¼å®¹æŸ¥è¯¢
            cidoc_dict = {}
            
            for key, value in artifact.items():
                # åªå¤„ç†æ¨¡æ¿ä¸­å®šä¹‰çš„å­—æ®µ
                # å°è¯•ç›´æ¥åŒ¹é…æˆ–å½’ä¸€åŒ–åŒ¹é…
                meta = None
                if key in field_metadata:
                    meta = field_metadata[key]
                else:
                    # å°è¯•æ¨¡ç³ŠåŒ¹é… metadata key
                    import re
                    def clean_string(s): return re.sub(r'\s+', '', str(s)).lower()
                    clean_k = clean_string(key)
                    for mk, mv in field_metadata.items():
                        if clean_string(mk) == clean_k:
                            meta = mv
                            break
                
                if meta:
                    cidoc_dict[key] = {
                        "value": value,
                        "entity_type": meta.get('entity_type'),
                        "property": meta.get('property'),
                        "target_class": meta.get('class')
                    }
                        
            cidoc_json = json.dumps(cidoc_dict, ensure_ascii=False)
            
            # 3. æ˜ å°„å­—æ®µ
            mapped = field_mapper.map_artifact_fields(artifact)
            
            # 4. æ·»åŠ æ–°å­—æ®µ
            mapped['raw_attributes'] = raw_data
            mapped['cidoc_attributes'] = cidoc_json
            
            # ä¿ç•™åŸå§‹æ•°æ®ä»¥ä¾¿ç”Ÿæˆä¸‰å…ƒç»„ï¼ˆå› ä¸ºmappedåçš„keyæ˜¯è‹±æ–‡ï¼Œå¯èƒ½ä¸¢å¤±åŸå§‹ä¸­æ–‡keyå¯¼è‡´åŒ¹é…å¤±è´¥ï¼‰
            mapped['#original_data'] = artifact 
            
            mapped_artifacts.append(mapped)
            
        self.db.add_log(task_id, 'INFO', f'æ•°æ®å¤„ç†å®Œæˆ')
        
        # å…³è”å›¾ç‰‡
        self.db.add_log(task_id, 'INFO', f'å…³è”å›¾ç‰‡...')
        img_manager = ImageManager(report_folder)
        # V3.7: å¢åŠ å›¾ç‰‡èµ„æºæ—¥å¿—
        if hasattr(img_manager, 'content_list') and img_manager.content_list is not None:
             self.db.add_log(task_id, 'INFO', f'ImageManageråŠ è½½äº† {len(img_manager.content_list)} ä¸ªå†…å®¹é¡¹')
        else:
             self.db.add_log(task_id, 'WARNING', 'ImageManageræœªèƒ½åŠ è½½content_listï¼Œå›¾ç‰‡å…³è”å¯èƒ½å—é™')
        
        img_linker = ImageLinker(img_manager)
        
        # æ’å…¥æ•°æ®åº“
        total_triples_count = 0
        linked_images_count = 0 # Track linked images
        
        for artifact in mapped_artifacts:
            # æå–åŸå§‹æ•°æ®
            original_data = artifact.pop('#original_data', {})
            
            # æ’å…¥æ–‡ç‰©
            if artifact_type == 'pottery':
                # é™¶å™¨æ¸…æ´—è§„åˆ™ï¼š
                # 1. æ’é™¤æ˜ç¡®æ ‡è®°ä¸ºç‰æ–™çš„ (jade_type å­˜åœ¨ä¸”ä¸ä¸ºç©º)
                # 2. æ’é™¤åˆ†ç±»æ˜¯"ç‰å™¨"çš„ (å¦‚æœ category_level1 å­˜åœ¨)
                if artifact.get('jade_type'):
                    self.db.add_log(task_id, 'WARNING', f"å‰”é™¤é”™è¯¯æ•°æ®: åœ¨é™¶å™¨ä»»åŠ¡ä¸­å‘ç°ç‰å™¨ç‰¹å¾ ({artifact.get('artifact_code')}, jade_type={artifact.get('jade_type')})")
                    continue
                    
                artifact_id = self.db.insert_pottery(artifact)
                
            elif artifact_type == 'jade':
                # ç‰å™¨æ¸…æ´—è§„åˆ™ï¼š
                # 1. æ’é™¤æ˜ç¡®æ ‡è®°ä¸ºé™¶åœŸçš„ (clay_type å­˜åœ¨ä¸”ä¸ä¸ºç©º)
                # 2. æ’é™¤åˆ†ç±»æ˜ç¡®ä¸ºé™¶å™¨çš„ (category_level1 åŒ…å« 'é™¶')
                # 3. æ’é™¤ç‰æ–™ç±»å‹ä¸ºé™¶çš„ (jade_type åŒ…å« 'é™¶')
                
                # æ£€æŸ¥ clay_type
                if artifact.get('clay_type'):
                    self.db.add_log(task_id, 'WARNING', f"å‰”é™¤é”™è¯¯æ•°æ®: åœ¨ç‰å™¨ä»»åŠ¡ä¸­å‘ç°é™¶å™¨ç‰¹å¾ ({artifact.get('artifact_code')}, clay_type={artifact.get('clay_type')})")
                    continue
                
                # æ£€æŸ¥ category_level1
                cat1 = str(artifact.get('category_level1', '') or '')
                if 'é™¶' in cat1:
                     self.db.add_log(task_id, 'WARNING', f"å‰”é™¤é”™è¯¯æ•°æ®: åœ¨ç‰å™¨ä»»åŠ¡ä¸­å‘ç°é™¶å™¨åˆ†ç±» ({artifact.get('artifact_code')}, category={cat1})")
                     continue

                # æ£€æŸ¥ jade_type (æ’é™¤ "é™¶" ä½†å…è®¸ "é™¶åœŸ" å‡ºç°åœ¨æè¿°ä¸­? ä¸ï¼Œç‰å™¨è¡¨ä¸åº”è¯¥å‡ºç°é™¶æè´¨)
                j_type = str(artifact.get('jade_type', '') or '')
                # å¦‚æœ jade_type æ˜¯ "é™¶" æˆ–è€…åŒ…å« "é™¶å™¨"
                if j_type == 'é™¶' or 'é™¶å™¨' in j_type:
                     self.db.add_log(task_id, 'WARNING', f"å‰”é™¤é”™è¯¯æ•°æ®: åœ¨ç‰å™¨ä»»åŠ¡ä¸­å‘ç°éç‰æè´¨ ({artifact.get('artifact_code')}, jade_type={j_type})")
                     continue
                
                # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœ artifact_code å’Œé™¶å™¨è¡¨é‡Œçš„é‡å¤ï¼Œä¸”é™¶å™¨è¡¨é‡Œå·²æœ‰ clay_typeï¼Œé‚£è¿™ä¸ªå¾ˆå¯èƒ½æ˜¯è¯¯åˆ¤
                # (è¿™ä¸ªæ£€æŸ¥æ¯”è¾ƒè€—æ—¶ä¸”é€»è¾‘å¤æ‚ï¼Œæš‚ä¸å®ç°ï¼Œå…ˆä¾èµ– clay_type å­—æ®µè¿‡æ»¤)
                
                artifact_id = self.db.insert_jade(artifact)
            else:
                continue
            
            # V3.6: ç”Ÿæˆå¹¶æ’å…¥ä¸‰å…ƒç»„ (ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œç¡®ä¿èƒ½åŒ¹é…åˆ°ä¸­æ–‡Template Key)
            # åŒæ—¶ä¹Ÿæ”¯æŒ English Key (å› ä¸º _generate_triples ç°åœ¨æ”¯æŒåŒå‘åŒ¹é…)
            if artifact_id:
                 self._generate_triples(original_data, artifact_type, artifact_id, task_id)

            # å…³è”å›¾ç‰‡
            try:
                images = img_linker.link_artifact_to_images(artifact, artifact_type)
                if images:
                    linked_images_count += len(images)
                    
                for img in images:
                    # æŸ¥æ‰¾image_id
                    cursor = self.db.conn.cursor()
                    cursor.execute(
                        'SELECT id FROM images WHERE task_id = ? AND image_hash = ?',
                        (task_id, img['image_hash'])
                    )
                    row = cursor.fetchone()
                    if row:
                        image_id = row[0]
                        self.db.link_artifact_to_image({
                            'artifact_type': artifact_type,
                            'artifact_id': artifact_id,
                            'artifact_code': artifact.get('artifact_code', ''),
                            'image_id': image_id,
                            'image_role': img['image_role'],
                            'display_order': img['display_order'],
                            'confidence': img['confidence']
                        })
                
                # æ›´æ–°has_imagesæ ‡å¿—
                if images:
                    # Re-query to get the main image ID if needed, or just use the first one found above
                    # Optimize: use the ID from the loop if possible
                    # For now, keep logic simple but robust
                    cursor = self.db.conn.cursor()
                    cursor.execute(
                        'SELECT id FROM images WHERE task_id = ? AND image_hash = ?',
                        (task_id, images[0]['image_hash'])
                    )
                    row = cursor.fetchone()
                    
                    table_name = 'pottery_artifacts' if artifact_type == 'pottery' else 'jade_artifacts'
                    self.db.conn.execute(
                        f'UPDATE {table_name} SET has_images = 1, main_image_id = ? WHERE id = ?',
                        (row[0] if row else None, artifact_id)
                    )
                    self.db.conn.commit()
                    
            except Exception as e:
                self.db.add_log(task_id, 'WARNING', f'å›¾ç‰‡å…³è”å¤±è´¥: {str(e)}')
        
        self.db.add_log(task_id, 'INFO', f'å›¾ç‰‡å…³è”å®Œæˆ: å…±å…³è” {linked_images_count} å¼ ')
        
        return len(merged_artifacts)
    
    def get_task_report(self, task_id: str) -> Dict:
        """è·å–ä»»åŠ¡æŠ¥å‘Š"""
        return self.db.get_task_summary(task_id)
    
    def close(self):
        """å…³é—­å·¥ä½œæµ"""
        self.db.close()


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰§è¡Œæ–‡ç‰©æŠ½å–å·¥ä½œæµ')
    parser.add_argument('--report', required=True, help='æŠ¥å‘Šæ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--pottery-template', help='é™¶å™¨æ¨¡æ¿è·¯å¾„')
    parser.add_argument('--jade-template', help='ç‰å™¨æ¨¡æ¿è·¯å¾„')
    parser.add_argument('--site-template', help='é—å€æ¨¡æ¿è·¯å¾„')
    parser.add_argument('--period-template', help='æ—¶æœŸæ¨¡æ¿è·¯å¾„')
    
    args = parser.parse_args()
    
    # æ„å»ºæ¨¡æ¿æ˜ å°„
    templates = {}
    if args.pottery_template:
        templates['pottery'] = args.pottery_template
    if args.jade_template:
        templates['jade'] = args.jade_template
    if args.site_template:
        templates['site'] = args.site_template
    if args.period_template:
        templates['period'] = args.period_template
    
    # æ‰§è¡Œå·¥ä½œæµ
    workflow = ExtractionWorkflow()
    
    try:
        print(f"å¼€å§‹æŠ½å–: {args.report}")
        task_id = workflow.execute_full_extraction(args.report, templates)
        print(f"\nâœ… æŠ½å–å®Œæˆï¼ä»»åŠ¡ID: {task_id}")
        
        # æ˜¾ç¤ºæŠ¥å‘Š
        report = workflow.get_task_report(task_id)
        print(f"\nä»»åŠ¡æŠ¥å‘Š:")
        print(f"  é—å€: {report['site']['site_name'] if report.get('site') else 'æœªæŠ½å–'}")
        print(f"  é™¶å™¨: {report['total_pottery']}ä»¶ (å«å›¾ç‰‡: {report['pottery_with_images']})")
        print(f"  ç‰å™¨: {report['total_jade']}ä»¶ (å«å›¾ç‰‡: {report['jade_with_images']})")
        print(f"  å›¾ç‰‡: {report['total_images']}å¼ ")
        
    finally:
        workflow.close()

